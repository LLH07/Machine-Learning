{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "This step deals with cleansing the consolidated text to remove noise to ensure efficient syntactic, semantic text analysis for deriving meaningful insights from text. Some common cleaning steps are briefed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenize\n",
    "\n",
    "Tokenizing is the process of breaking a large set of texts into smaller meaningful chunks such as sentences, words, phrases. NLTK library provides sent_tokenize for sentence level tokenizing, which uses a pre-trained model PunktSentenceTokenize, to determine punctuation and characters marking the end of sentence for European languages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics skills, and programming skills are equally important for analytics.', 'Statistics skills, and domain knowledge are important for analytics.', 'I like reading books and travelling.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text='Statistics skills, and programming skills are equally important for analytics. Statistics skills, and domain knowledge are important for analytics. I like reading books and travelling.'\n",
    "\n",
    "# sent_tokenize uses an instance of PunktSentenceTokenizer from the nltk. tokenize.punkt module. This instance has already been trained on and works well for many European languages. So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence.\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola.', 'Esta es una frase espanola.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are total 17 european languages that NLTK support for sentence tokenize\n",
    "# Let's try loading a spanish model\n",
    "import nltk.data\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola. Esta es una frase espanola.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenize\n",
    "\n",
    "word_tokenize is a wrapper function that calls tokenize by the TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n",
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print (word_tokenize(text))\n",
    "\n",
    "# Another equivalent call method\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print (tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "# Except the TreebankWordTokenizer, there are other alternative word tokenizers, such as PunktWordTokenizer and WordPunktTokenizer\n",
    "# PunktTokenizer splits on punctuation, but keeps it with the word\n",
    "# from nltk.tokenize import PunktWordTokenizer\n",
    "# punkt_word_tokenizer = PunktWordTokenizer()\n",
    "# print punkt_word_tokenizer.tokenize(text) \n",
    "\n",
    "# WordPunctTokenizer splits all punctuations into separate tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print (word_punct_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PoS tagging\n",
    "\n",
    "The default pos tagger model using in NLTK is maxent_treebanck_pos_tagger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk\n",
    "\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize('This is a sample English sentence'))\n",
    "print (tagged_sent)\n",
    "\n",
    "tree = chunk.ne_chunk(tagged_sent)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "# To get help about tags\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "PT = PerceptronTagger()\n",
    "print (PT.tag('This is a sample English sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample English sentence\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    "\n",
    "print (remove_stopwords('This is a sample English sentence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample English sentence with punctuations\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "# Function to remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    "\n",
    "print (remove_punctuations('This is a sample English sentence, with punctuations!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove whitespace & numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This \tis a     sample  English   sentence, \n",
      " with whitespace and numbers 1234!\n",
      "Removed whitespace:  This is a sample English sentence, with whitespace and numbers 1234!\n",
      "Removed numbers:  This \tis a     sample  English   sentence, \n",
      " with whitespace and numbers !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "text = 'This \tis a     sample  English   sentence, \\n with whitespace and numbers 1234!'\n",
    "print ('Original Text: ', text)\n",
    "print ('Removed whitespace: ', remove_whitespace(text))\n",
    "print ('Removed numbers: ', remove_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "It is the process of transforming to the root word i.e., it uses an algorithm that removes common word endings for English words, such as “ly”, “es”, “ed” and “s”. For example, assuming for an analysis you may want to consider “carefully”, “cared”, “cares”, “caringly” as “care” instead of separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "image/png": {
       "width": 500
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../Chapter 5 Figures/Stemmers.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  caring cares cared caringly carefully\n",
      "Porter:  [b'care', b'care', b'care', b'caringli', b'care']\n",
      "Lancaster:  [b'car', b'car', b'car', b'car', b'car']\n",
      "Snowball:  [b'care', b'care', b'care', b'care', b'care']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Function to apply stemming to a list of words\n",
    "def words_stemmer(words, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\"):\n",
    "    supported_stemmers = [\"PorterStemmer\",\"LancasterStemmer\",\"SnowballStemmer\"]\n",
    "    if type is False or type not in supported_stemmers:\n",
    "        return words\n",
    "    else:\n",
    "        stem_words = []\n",
    "        if type == \"PorterStemmer\":\n",
    "            stemmer = PorterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        if type == \"LancasterStemmer\":\n",
    "            stemmer = LancasterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        if type == \"SnowballStemmer\":\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        return stem_words\n",
    "    \n",
    "words =  'caring cares cared caringly carefully'  \n",
    "\n",
    "print (\"Original: \", words)\n",
    "print (\"Porter: \", words_stemmer(nltk.word_tokenize(words), \"PorterStemmer\"))\n",
    "print (\"Lancaster: \", words_stemmer(nltk.word_tokenize(words), \"LancasterStemmer\"))\n",
    "print (\"Snowball: \", words_stemmer(nltk.word_tokenize(words), \"SnowballStemmer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "\n",
    "It is the process of transforming to the dictionary base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized:  [b'care', b'care', b'care', b'caringly', b'carefully']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos = find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos).encode(encoding))\n",
    "    return lemma_words\n",
    "\n",
    "# Function to find part of speech tag for a word\n",
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    # You can learn more about these at http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html#sect3\n",
    "    # You can learn more about all the penn tree tags at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags - 'JJ', 'JJR', 'JJS'    \n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v': \n",
    "        return 'v'\n",
    "    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "print (\"Lemmatized: \", words_lemmatizer(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above case, 'caringly' / 'carefully' are inflected form of care and they are an entry word listed in WordNet Dictoinary so they are retained in their actual form itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition:  benefit\n",
      "Example:  ['for your own good', \"what's the good of worrying?\"]\n",
      "synonyms: \n",
      " {'effective', 'undecomposed', 'estimable', 'skillful', 'practiced', 'safe', 'near', 'just', 'honest', 'secure', 'salutary', 'respectable', 'well', 'in_effect', 'unspoiled', 'dear', 'skilful', 'expert', 'sound', 'in_force', 'good', 'proficient', 'right', 'serious', 'beneficial', 'dependable', 'ripe', 'upright', 'full', 'unspoilt', 'goodness', 'trade_good', 'commodity', 'adept', 'thoroughly', 'honorable', 'soundly'}\n",
      "antonyms: \n",
      " {'badness', 'ill', 'evilness', 'bad', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"good\")\n",
    "print (\"Definition: \", syns[0].definition())\n",
    "print (\"Example: \", syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "# Print  synonums and antonyms (having opposite meaning words)\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print (\"synonyms: \\n\", set(synonyms))\n",
    "print (\"antonyms: \\n\", set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['This', 'is', 'a', 'sample', 'English', 'sentence']\n",
      "2-gram:  ['This is', 'is a', 'a sample', 'sample English', 'English sentence']\n",
      "3-gram:  ['This is a', 'is a sample', 'a sample English', 'sample English sentence']\n",
      "4-gram:  ['This is a sample', 'is a sample English', 'a sample English sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Function to extract n-grams from text\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]  \n",
    "\n",
    "text = 'This is a sample English sentence'\n",
    "\n",
    "print (\"1-gram: \", get_ngrams(text, 1))\n",
    "print (\"2-gram: \", get_ngrams(text, 2))\n",
    "print (\"3-gram: \", get_ngrams(text, 3))\n",
    "print (\"4-gram: \", get_ngrams(text, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract bigram and count their respective frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  dict_keys(['Statistics skills', 'skills and', 'and programming', 'programming skills', 'skills are', 'are equally', 'equally important', 'important for', 'for analytics', 'analytics Statistics', 'and domain', 'domain knowledge', 'knowledge are', 'are important'])\n",
      "\n",
      "Frequency:  dict_values([2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1])\n",
      "                      frequency\n",
      "Statistics skills             2\n",
      "skills and                    2\n",
      "and programming               1\n",
      "programming skills            1\n",
      "skills are                    1\n",
      "are equally                   1\n",
      "equally important             1\n",
      "important for                 2\n",
      "for analytics                 2\n",
      "analytics Statistics          1\n",
      "and domain                    1\n",
      "domain knowledge              1\n",
      "knowledge are                 1\n",
      "are important                 1\n"
     ]
    }
   ],
   "source": [
    "text = 'Statistics skills, and programming skills are equally important for analytics. Statistics skills, and domain knowledge are important for analytics'\n",
    "\n",
    "# remove punctuations\n",
    "text = remove_punctuations(text)\n",
    "\n",
    "# Extracting bigrams\n",
    "result = get_ngrams(text,2)\n",
    "\n",
    "# Counting bigrams\n",
    "result_count = Counter(result)\n",
    "\n",
    "print (\"Words: \", result_count.keys()) # Bigrams\n",
    "print (\"\\nFrequency: \", result_count.values()) # Bigram frequency\n",
    "\n",
    "# Converting to the result to a data frame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(result_count, orient='index')\n",
    "df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index and column name\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Doc_2.txt  Doc_3.txt  Doc_1.txt\n",
      "analytics            1          0          1\n",
      "and                  1          1          1\n",
      "are                  1          0          1\n",
      "books                0          1          0\n",
      "domain               1          0          0\n",
      "equally              0          0          1\n",
      "for                  1          0          1\n",
      "important            1          0          1\n",
      "knowledge            1          0          0\n",
      "like                 0          1          0\n",
      "programming          0          0          1\n",
      "reading              0          1          0\n",
      "skills               1          0          2\n",
      "statistics           1          0          1\n",
      "travelling           0          1          0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Create a dictionary with key as file names and values as text for all files in a given folder\n",
    "def CorpusFromDir(dir_path):\n",
    "    result = dict(docs = [open(os.path.join(dir_path,f)).read() for f in os.listdir(dir_path)],\n",
    "               ColNames = map(lambda x: x, os.listdir(dir_path)))\n",
    "    return result\n",
    "\n",
    "docs = CorpusFromDir('Data/text_files/')\n",
    "\n",
    "# Initialize\n",
    "vectorizer = CountVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "\n",
    "#create dataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index = vectorizer.get_feature_names())\n",
    "\n",
    "# Change column headers to be file names\n",
    "df.columns = docs.get('ColNames')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "In the area of information retrieval TF-IDF is a good statistical measure to reflect the relevance of term to the document in a collection of documents or corpus. Let’s break TF_IDF and apply example to understand it better.\n",
    "\n",
    "TF (term) =  (Number of times term appears in a document)/(Total number of terms in the document)\n",
    "IDF (term) = log⁡(  (Total number of documents)/(Number of documents with a given term in it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Doc_2.txt  Doc_3.txt  Doc_1.txt\n",
      "analytics     0.315269   0.000000   0.276703\n",
      "and           0.244835   0.283217   0.214884\n",
      "are           0.315269   0.000000   0.276703\n",
      "books         0.000000   0.479528   0.000000\n",
      "domain        0.414541   0.000000   0.000000\n",
      "equally       0.000000   0.000000   0.363831\n",
      "for           0.315269   0.000000   0.276703\n",
      "important     0.315269   0.000000   0.276703\n",
      "knowledge     0.414541   0.000000   0.000000\n",
      "like          0.000000   0.479528   0.000000\n",
      "programming   0.000000   0.000000   0.363831\n",
      "reading       0.000000   0.479528   0.000000\n",
      "skills        0.315269   0.000000   0.553405\n",
      "statistics    0.315269   0.000000   0.276703\n",
      "travelling    0.000000   0.479528   0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = CorpusFromDir('Data/text_files/')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "\n",
    "#create dataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index = vectorizer.get_feature_names())\n",
    "\n",
    "# Change column headers to be file names\n",
    "df.columns = docs.get('ColNames')\n",
    "print (df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
